{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAowIwaVcNO_"
      },
      "source": [
        "**This is the notebook that demonstrates how to classify labels images using KNN algorithm**\n",
        "\n",
        "This notebook loads the animal dataset from the google drive.Then applies KNN algorithm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLDX6g5mb8ys"
      },
      "source": [
        "![alt text](http://csopensource.com/wp-content/uploads/2018/07/knnmeme.jpeg)\n",
        "\n",
        "---\n",
        "\n",
        "The k-Nearest Neighbor classifier is by far the most simple machine learning and image classi-\n",
        "fication algorithm. \n",
        "It doesn’t actually “**learn**” anything. Instead, this\n",
        "algorithm directly relies on the **distance between feature vectors** (which in our case, are the raw\n",
        "RGB pixel intensities of the images).\n",
        "\n",
        "k-NN algorithm classifies unknown data points by finding the most common\n",
        "class among the k closest examples. Each data point in the k closest data points casts a vote, and the\n",
        "category with the highest number of votes wins. \n",
        "Or, in plain English: “***Tell me who your neighbors are, and I’ll tell you who you are***”"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0bGOzV_DfzDC"
      },
      "source": [
        "K- Nearest Neighbors is a\n",
        "\n",
        "> **Supervised machine learning algorithm**\n",
        "\n",
        "> **Non parametric** as it **does not** make an **assumption** about the **underlying data distribution pattern**\n",
        "\n",
        "\n",
        "> It doesnt have the training step,here **K stands for Number of neighbours**.It uses **distance metric **like **L1 ,L2 distance** to predict the label of new point into N-dimensional space\n",
        "\n",
        "**Pros**:\n",
        "\n",
        "\n",
        "1.   Learns complex models easily.\n",
        "2.   Robust to noisy data,\n",
        "3    No training phase involved as it direclty relies on labels of K nearest neighbours\n",
        "4    Effective if training set is large\n",
        "5   Classifying a new testing point requires a comparison to every single data point in our training data, which scales O(N), making working with larger datasets computationally prohibitive.\n",
        "\n",
        "\n",
        "**Cons**\n",
        "\n",
        "1.   Difficult to choose value of K in this approach\n",
        "2.   Difficut to estimate which distance could give best result.\n",
        "3    Not effective if data has high dimensional since large storage is required,low computational efficiency ,data sparsity\n",
        ",false intuition,close nearest neighbours  becomes less relevant\n",
        "4. Works well if data is low dimensional\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kk3Nqvh9ij0e"
      },
      "source": [
        "We will load the dataset from the google drive. For that we import google colab library ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIBlZ2JZKek",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "38508a66-2c7d-4021-8b71-ae4b1adb306d"
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ8R7qPSjA0a"
      },
      "source": [
        "Lets **list** the folder in our dataset ,here the folder is animals"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGH5vImXvQ6Y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "426e1e69-c871-4808-c196-77140e40f329"
      },
      "source": [
        "!ls /content/gdrive/My\\ Drive/datasets/animals"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cats  dogs  panda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TrrpRKtIjV0b"
      },
      "source": [
        "Here is the class SimpleDatasetLoader which loads the dataset fom the drive and it gives data and label,which are tuple of numpy array of data and labels\n",
        "\n",
        "# Implement SimpleDatasetLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OH__VfWsR3ue"
      },
      "source": [
        "#Class to load the dataset images from drivce\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class SimpleDatasetLoader:\n",
        "    # Method: Constructor\n",
        "    def __init__(self, preprocessors=None):\n",
        "        \"\"\"\n",
        "        :param preprocessors: List of image preprocessors\n",
        "        \"\"\"\n",
        "        self.preprocessors = preprocessors\n",
        "\n",
        "        if self.preprocessors is None:\n",
        "            self.preprocessors = []\n",
        "\n",
        "    # Method: Used to load a list of images for pre-processing\n",
        "    def load(self, image_paths, verbose=-1):\n",
        "        \"\"\"\n",
        "        :param image_paths: List of image paths\n",
        "        :param verbose: Parameter for printing information to console\n",
        "        :return: Tuple of data and labels\n",
        "        \"\"\"\n",
        "        data, labels = [], []\n",
        "\n",
        "        for i, image_path in enumerate(image_paths):\n",
        "            image = cv2.imread(image_path)\n",
        "            label = image_path.split(os.path.sep)[-2]\n",
        "\n",
        "            if self.preprocessors is not None:\n",
        "                for p in self.preprocessors:\n",
        "                    image = p.preprocess(image)\n",
        "\n",
        "            data.append(image)\n",
        "            labels.append(label)\n",
        "\n",
        "            if verbose > 0 and i > 0 and (i+1) % verbose == 0:\n",
        "                print('[INFO]: Processed {}/{}'.format(i+1, len(image_paths)))\n",
        "\n",
        "        return (np.array(data), np.array(labels))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BlK-S-PlZ9n"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Machine learning algorithm such as k-NN require all images in a dataset to have a **fixed feature vector size**.\n",
        "\n",
        "In the case of images, this\n",
        "requirement implies that our images must be preprocessed and scaled to have identical widths and heights.\n",
        "\n",
        "There are a number of ways to accomplish this resizing and scaling, ranging from more advanced methods that respect the aspect ratio of the original image to the scaled image to simple methods that ignore the aspect ratio and simply squash the width and height to the required dimensions\n",
        "\n",
        "class SimplePreprocessor builds an image preprocessor that resizes\n",
        "the image, ignoring the aspect ratio.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "umz4t5kxmg-c"
      },
      "source": [
        "#Implementing SimplePreprocessor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3b8mRpIbR9Tf"
      },
      "source": [
        "#Class Preprocessror \n",
        "class SimplePreprocessor:\n",
        "    # Method: Constructor\n",
        "    def __init__(self, width, height, interpolation=cv2.INTER_AREA):\n",
        "        \"\"\"\n",
        "        :param width: Image width\n",
        "        :param height: Image height\n",
        "        :param interpolation: Interpolation algorithm\n",
        "        \"\"\"\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    # Method: Used to resize the image to a fixed size (ignoring the aspect ratio)\n",
        "    def preprocess(self, image):\n",
        "        \"\"\"\n",
        "        :param image: Image\n",
        "        :return: Re-sized image\n",
        "        \"\"\"\n",
        "        return cv2.resize(image, (self.width, self.height), interpolation=self.interpolation)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7obCyoDtmIsd"
      },
      "source": [
        "# Implementing k-NN\n",
        "\n",
        "---\n",
        "• Step #1 –** Gather Our Dataset**: The datasets consists of 3,000 images with 1,000\n",
        "images per dog, cat, and panda class, respectively. Each image is represented in the RGB76\n",
        "color space. We will preprocess each image by resizing it to 32 × 32 pixels. Taking into\n",
        "account the three RGB channels, the resized image dimensions imply that each image in the\n",
        "dataset is represented by 32 × 32 × 3 = 3, 072 integers.\n",
        "\n",
        "• Step #2 – **Split the Dataset**: We will split the data, One split for training, and the other for testing. \n",
        "• Step #3 – **Train the Classifier**: Our k-NN classifier will be trained on the raw pixel intensi-\n",
        "ties of the images in the training set.\n",
        "\n",
        "• Step #4 – **Evaluate**: Once our k-NN classifier is trained, we can evaluate performance on\n",
        "the test set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcCvmbWGSDhk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "c981f3f9-2881-4bfc-edaf-a7c182c684d2"
      },
      "source": [
        "\n",
        "from imutils import paths\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from __main__ import SimplePreprocessor\n",
        "from __main__ import SimpleDatasetLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = list(paths.list_images(\"/content/gdrive/My Drive/datasets/animals\"))\n",
        "\n",
        "# Initialize SimplePreprocessor and SimpleDatasetLoader and load data and labels\n",
        "print('[INFO]: Images loading....')\n",
        "sp = SimplePreprocessor(32, 32)\n",
        "sdl = SimpleDatasetLoader(preprocessors=[sp])\n",
        "(data, labels) = sdl.load(image_paths, verbose=500)\n",
        "\n",
        "# Reshape from (3000, 32, 32, 3) to (3000, 32*32*3=3072)\n",
        "data = data.reshape((data.shape[0], 3072))\n",
        "\n",
        "# Print information about memory consumption\n",
        "print('[INFO]: Features Matrix: {:.1f}MB'.format(float(data.nbytes / 1024*1000.0)))\n",
        "\n",
        "# Encode labels as integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Split data into training (75%) and testing (25%) data\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and evaluate the k-NN classifier on the raw pixel intensities\n",
        "print('[INFO]: Classification starting....')\n",
        "model = KNeighborsClassifier(n_neighbors=7,\n",
        "                             n_jobs=1)\n",
        "model.fit(train_x, train_y)\n",
        "print(classification_report(test_y, model.predict(test_x),\n",
        "                            target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Images loading....\n",
            "[INFO]: Processed 500/3000\n",
            "[INFO]: Processed 1000/3000\n",
            "[INFO]: Processed 1500/3000\n",
            "[INFO]: Processed 2000/3000\n",
            "[INFO]: Processed 2500/3000\n",
            "[INFO]: Processed 3000/3000\n",
            "[INFO]: Features Matrix: 9000000.0MB\n",
            "[INFO]: Classification starting....\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.44      0.67      0.53       239\n",
            "        dogs       0.43      0.49      0.46       249\n",
            "       panda       0.93      0.39      0.54       262\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       750\n",
            "   macro avg       0.60      0.51      0.51       750\n",
            "weighted avg       0.61      0.51      0.51       750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qQVucJV3DWx"
      },
      "source": [
        "# How to find Best K?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbjd3lACpnSo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "bf40b8f3-8723-41d6-eb76-ff6939556208"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import numpy as np \n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=2,n_jobs=1)\n",
        "model.fit(train_x, train_y)\n",
        "\n",
        "accuracy = accuracy_score(model.predict(test_x), test_y)\n",
        "print(accuracy)\n",
        "n_neighbors = np.array([7,8,9,10,12,15,20])\n",
        "param_grid = dict(n_neighbors=n_neighbors)\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid)\n",
        "grid.fit(train_x, train_y)\n",
        "print(grid.best_score_)\n",
        "print(grid.best_estimator_.n_neighbors)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.4146666666666667\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.47688888888888886\n",
            "20\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jrUCePZnzYoZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ddffbd75-bd30-4ce5-c454-51cb26eba7d9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#print(grid.cv_results_)\n",
        "print(grid.param_grid)\n",
        "print(grid.best_score_)\n",
        "print(grid.scorer_)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'n_neighbors': array([ 7,  8,  9, 10, 12, 15, 20])}\n",
            "0.47688888888888886\n",
            "<function _passthrough_scorer at 0x7fe7edff9d90>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IosHrvJAjzbz"
      },
      "source": [
        "K=20 Neighbours gives best score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10Q7T9-Dsjq4"
      },
      "source": [
        "**Classifying a new testing point\n",
        "requires a comparison to every single data point in our training data, which scales O(N), making\n",
        "working with larger datasets computationally prohibitive.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEDFNJzlrt0t"
      },
      "source": [
        "# How to make KNN Faster?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_kWN9QJ0m9t",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "0a646aae-cb61-4dab-c966-cd2ddd2dfac3"
      },
      "source": [
        " #algorithm='ball_tree'\n",
        " #its fs\n",
        "  \n",
        "from imutils import paths\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from __main__ import SimplePreprocessor\n",
        "from __main__ import SimpleDatasetLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Get list of image paths\n",
        "image_paths = list(paths.list_images(\"/content/gdrive/My Drive/datasets/animals\"))\n",
        "\n",
        "# Initialize SimplePreprocessor and SimpleDatasetLoader and load data and labels\n",
        "print('[INFO]: Images loading....')\n",
        "sp = SimplePreprocessor(32, 32)\n",
        "sdl = SimpleDatasetLoader(preprocessors=[sp])\n",
        "(data, labels) = sdl.load(image_paths, verbose=500)\n",
        "\n",
        "# Reshape from (3000, 32, 32, 3) to (3000, 32*32*3=3072)\n",
        "data = data.reshape((data.shape[0], 3072))\n",
        "\n",
        "# Print information about memory consumption\n",
        "print('[INFO]: Features Matrix: {:.1f}MB'.format(float(data.nbytes / 1024*1000.0)))\n",
        "\n",
        "# Encode labels as integers\n",
        "le = LabelEncoder()\n",
        "labels = le.fit_transform(labels)\n",
        "\n",
        "# Split data into training (75%) and testing (25%) data\n",
        "(train_x, test_x, train_y, test_y) = train_test_split(data, labels, test_size=0.25, random_state=42)\n",
        "\n",
        "# Train and evaluate the k-NN classifier on the raw pixel intensities\n",
        "print('[INFO]: Classification starting....')\n",
        "model = KNeighborsClassifier(n_neighbors=7,\n",
        "                             n_jobs=1,algorithm='kd_tree')\n",
        "model.fit(train_x, train_y)\n",
        "print(classification_report(test_y, model.predict(test_x),\n",
        "                            target_names=le.classes_))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[INFO]: Images loading....\n",
            "[INFO]: Processed 500/3000\n",
            "[INFO]: Processed 1000/3000\n",
            "[INFO]: Processed 1500/3000\n",
            "[INFO]: Processed 2000/3000\n",
            "[INFO]: Processed 2500/3000\n",
            "[INFO]: Processed 3000/3000\n",
            "[INFO]: Features Matrix: 9000000.0MB\n",
            "[INFO]: Classification starting....\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "        cats       0.44      0.67      0.53       239\n",
            "        dogs       0.43      0.49      0.46       249\n",
            "       panda       0.93      0.39      0.54       262\n",
            "\n",
            "   micro avg       0.51      0.51      0.51       750\n",
            "   macro avg       0.60      0.51      0.51       750\n",
            "weighted avg       0.61      0.51      0.51       750\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPmFnw7q1Ip9"
      },
      "source": [
        "The construction of a KD tree is very fast: because **partitioning** is performed only along the data axes, no -dimensional distances need to be computed. Once constructed, the nearest neighbor of a query point can be determined with only  distance computations. Though the KD tree approach is **very fast for low-dimensional (O(logN)) n**eighbors searches, it becomes inefficient as  grows very large: this is one manifestation of the so-called “curse of dimensionality”. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EM8AgyAC1HpA"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}